{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGWHRFWxxQGa"
   },
   "source": [
    "# STARK Amazon SKB Data Extraction and Cleaning\n",
    "\n",
    "**Extract and process the STARK Amazon Semi-structured Knowledge Base (SKB) dataset**\n",
    "\n",
    "This notebook downloads the STARK Amazon dataset directly from Hugging Face, extracts node information including product attributes, reviews, and graph edge relationships (also_buy, also_view, has_brand, has_category, has_color), and performs data cleaning operations to create a structured CSV file suitable for analysis and machine learning tasks.\n",
    "\n",
    "## Output\n",
    "- Clean CSV file with 1M+ product nodes\n",
    "- Includes: title, description, features, brand, price, rank, reviews, ratings, categories, and edge relationships\n",
    "- Combined text field for NLP/embedding tasks\n",
    "\n",
    "## Requirements\n",
    "- Google Colab environment\n",
    "- Google Drive mounted\n",
    "- ~4GB download (cached after first run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2CiFXbCHuAv"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MINIMAL SETUP\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/snap-stanford/stark.git\n",
    "%cd stark\n",
    "\n",
    "# Install minimal dependencies\n",
    "print(\"Installing minimal dependencies...\")\n",
    "!pip install pandas huggingface_hub tqdm -q\n",
    "\n",
    "# Check numpy version\n",
    "import numpy as np\n",
    "print(f\"Current numpy version: {np.__version__}\")\n",
    "\n",
    "print(\"âœ… Minimal setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAxekICfHuEH"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load and extract SKB data\n",
    "# ============================================================================\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# Download the processed Amazon SKB data directly\n",
    "print(\"Downloading Amazon SKB data...\")\n",
    "repo_id = \"snap-stanford/stark\"\n",
    "filename = \"skb/amazon/processed.zip\"\n",
    "\n",
    "file_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\")\n",
    "print(f\"âœ… Downloaded to: {file_path}\")\n",
    "\n",
    "# Extract\n",
    "extract_dir = \"/content/amazon_skb\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "print(\"Extracting files...\")\n",
    "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"âœ… Extracted!\")\n",
    "print(f\"\\nContents of {extract_dir}:\")\n",
    "for root, dirs, files in os.walk(extract_dir):\n",
    "    level = root.replace(extract_dir, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:10]:  # Show first 10 files\n",
    "        print(f'{subindent}{file}')\n",
    "    if len(files) > 10:\n",
    "        print(f'{subindent}... and {len(files)-10} more files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu_UpTQRHuN4"
   },
   "outputs": [],
   "source": [
    "# Install torch\n",
    "!pip install torch -q\n",
    "\n",
    "print(\"âœ… Torch installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pg4tmnS9HuRU"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load Amazon SKB with edges and convert to CSV\n",
    "# ============================================================================\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define edge type mapping\n",
    "edge_type_dict = {\n",
    "    0: 'also_buy',\n",
    "    1: 'also_view',\n",
    "    2: 'has_brand',\n",
    "    3: 'has_category',\n",
    "    4: 'has_color'\n",
    "}\n",
    "\n",
    "# Load all the data files\n",
    "print(\"Loading data files...\")\n",
    "\n",
    "# Load node info\n",
    "with open('/content/amazon_skb/processed/node_info.pkl', 'rb') as f:\n",
    "    node_info = pickle.load(f)\n",
    "print(f\"âœ… Loaded {len(node_info)} nodes\")\n",
    "\n",
    "# Load edge index\n",
    "edge_index = torch.load('/content/amazon_skb/processed/edge_index.pt')\n",
    "print(f\"âœ… Loaded edge_index with shape {edge_index.shape}\")\n",
    "\n",
    "# Load edge types\n",
    "edge_types = torch.load('/content/amazon_skb/processed/edge_types.pt')\n",
    "print(f\"âœ… Loaded {len(edge_types)} edge types\")\n",
    "\n",
    "# Load edge type dict\n",
    "with open('/content/amazon_skb/processed/edge_type_dict.pkl', 'rb') as f:\n",
    "    edge_type_mapping = pickle.load(f)\n",
    "print(f\"âœ… Loaded edge type mapping: {edge_type_mapping}\")\n",
    "\n",
    "print(\"\\nBuilding edge dictionary...\")\n",
    "# Convert to numpy for faster processing\n",
    "edges = edge_index.numpy()\n",
    "edge_types_np = edge_types.numpy()\n",
    "edge_list = list(zip(edges[0], edges[1], edge_types_np))\n",
    "\n",
    "# Create a dictionary mapping each node to its edges by type\n",
    "edge_dict = {node_id: {etype: [] for etype in edge_type_dict.values()}\n",
    "             for node_id in node_info.keys()}\n",
    "\n",
    "for src, dst, etype in tqdm(edge_list, desc=\"Processing edges\"):\n",
    "    edge_name = edge_type_dict.get(etype, \"Unknown\")\n",
    "    if src in edge_dict:\n",
    "        edge_dict[src][edge_name].append(int(dst))\n",
    "    if dst in edge_dict:\n",
    "        edge_dict[dst][edge_name].append(int(src))\n",
    "\n",
    "print(f\"âœ… Built edge dictionary for {len(edge_dict)} nodes\")\n",
    "\n",
    "# Now build the dataframe with all information\n",
    "print(\"\\nBuilding DataFrame with node info, reviews, and edges...\")\n",
    "node_data = []\n",
    "\n",
    "for node_id, node_info_dict in tqdm(node_info.items(), desc=\"Processing nodes\"):\n",
    "    # Extract key fields\n",
    "    title = node_info_dict.get(\"title\", \"\")\n",
    "    description = node_info_dict.get(\"description\", \"\")\n",
    "    feature = node_info_dict.get(\"feature\", \"\")\n",
    "    global_category = node_info_dict.get(\"global_category\", \"\")\n",
    "    categories = \", \".join(node_info_dict.get(\"category\", [])) if isinstance(node_info_dict.get(\"category\"), list) else \"\"\n",
    "    brand = node_info_dict.get(\"brand\", \"\")\n",
    "    price = node_info_dict.get(\"price\", \"\")\n",
    "    rank = node_info_dict.get(\"rank\", \"\")\n",
    "\n",
    "    # Extract review details\n",
    "    reviews = node_info_dict.get(\"review\", [])\n",
    "    if isinstance(reviews, list):\n",
    "        review_texts = [str(review.get(\"reviewText\", \"\")) for review in reviews if isinstance(review, dict)]\n",
    "        review_ratings = [review.get(\"overall\", None) for review in reviews if isinstance(review, dict)]\n",
    "    else:\n",
    "        review_texts = []\n",
    "        review_ratings = []\n",
    "\n",
    "    # Get edges for this node\n",
    "    node_edges = edge_dict.get(node_id, {etype: [] for etype in edge_type_dict.values()})\n",
    "\n",
    "    # Store structured row\n",
    "    node_data.append({\n",
    "        \"node_id\": node_id,\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"feature\": feature,\n",
    "        \"global_category\": global_category,\n",
    "        \"categories\": categories,\n",
    "        \"brand\": brand,\n",
    "        \"price\": price,\n",
    "        \"rank\": rank,\n",
    "        \"reviews\": \" | \".join(filter(None, review_texts)),\n",
    "        \"ratings\": review_ratings,\n",
    "        \"also_buy\": node_edges.get(\"also_buy\", []),\n",
    "        \"also_view\": node_edges.get(\"also_view\", []),\n",
    "        \"has_brand\": node_edges.get(\"has_brand\", []),\n",
    "        \"has_category\": node_edges.get(\"has_category\", []),\n",
    "        \"has_color\": node_edges.get(\"has_color\", [])\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(node_data)\n",
    "\n",
    "print(f\"\\nâœ… Created DataFrame with {len(df)} rows and {len(df.columns)} columns\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "# Save to CSV\n",
    "output_path = '/content/drive/MyDrive/stark_amazon_skb_with_edges.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ SUCCESS! Saved to {output_path}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "display(df.head(3))\n",
    "print(f\"\\nDataFrame info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqX2_ZsOOQzp"
   },
   "source": [
    "### Cleaning Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQ_Wd7U2HuYJ"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load raw Amazon SKB with edges\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the raw CSV file you just created\n",
    "file_path = '/content/drive/MyDrive/stark_amazon_skb_with_edges.csv'\n",
    "node_df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"Loaded {len(node_df)} rows\")\n",
    "print(f\"\\nColumns: {list(node_df.columns)}\")\n",
    "display(node_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahHUDRv5Hscr"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Convert text columns to lowercase\n",
    "# ============================================================================\n",
    "\n",
    "text_columns = ['title', 'feature', 'description', 'global_category', 'categories', 'brand', 'reviews']\n",
    "\n",
    "for col in text_columns:\n",
    "    if col in node_df.columns:\n",
    "        node_df[col] = node_df[col].str.lower()\n",
    "\n",
    "print(\"Text normalized to lowercase\")\n",
    "display(node_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbuYlSdRHsfa"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Function to clean text - remove punctuation, brackets, extra spaces\n",
    "# ============================================================================\n",
    "\n",
    "import string\n",
    "\n",
    "def clean_text_field(desc):\n",
    "    if isinstance(desc, list):\n",
    "        desc = ' '.join(desc)\n",
    "    elif not isinstance(desc, str):\n",
    "        desc = \"\"\n",
    "    # Remove punctuation and brackets\n",
    "    desc = desc.translate(str.maketrans('', '', string.punctuation + \"[]\"))\n",
    "    # Replace multiple spaces with single space\n",
    "    return ' '.join(desc.split())\n",
    "\n",
    "# Clean description and feature\n",
    "node_df['description_cleaned'] = node_df['description'].apply(clean_text_field)\n",
    "node_df['feature_cleaned'] = node_df['feature'].apply(clean_text_field)\n",
    "\n",
    "# Drop original columns and rename cleaned ones\n",
    "node_df = node_df.drop(columns=['feature', 'description'])\n",
    "node_df = node_df.rename(columns={\n",
    "    'description_cleaned': 'description',\n",
    "    'feature_cleaned': 'feature'\n",
    "})\n",
    "\n",
    "print(\"Description and feature cleaned\")\n",
    "display(node_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oe-QsQqgHsiI"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Clean price column - remove non-numeric characters and convert to numeric\n",
    "# ============================================================================\n",
    "\n",
    "node_df['price'] = node_df['price'].replace(r'[^\\d.]', '', regex=True)\n",
    "node_df['price'] = node_df['price'].replace('', pd.NA)\n",
    "node_df['price'] = pd.to_numeric(node_df['price'], errors='coerce')\n",
    "\n",
    "print(\"Price format fixed\")\n",
    "display(node_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N4XHLPxF1mA"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Extract numeric part from rank, remove commas\n",
    "# ============================================================================\n",
    "\n",
    "import re\n",
    "\n",
    "node_df['rank_cleaned'] = node_df['rank'].apply(lambda x: str(x).replace(',', '')) \\\n",
    "                                         .apply(lambda x: re.findall(r'\\d+', x)) \\\n",
    "                                         .apply(lambda x: x[0] if x else \"Unknown\")\n",
    "\n",
    "# Drop original and rename\n",
    "node_df = node_df.drop(columns=['rank'])\n",
    "node_df = node_df.rename(columns={'rank_cleaned': 'rank'})\n",
    "\n",
    "print(\"Rank format fixed\")\n",
    "display(node_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tB2qPnSBF1qe"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Calculate avg_rating and rating_count from ratings column\n",
    "# ============================================================================\n",
    "\n",
    "node_df['avg_rating'] = node_df['ratings'].apply(\n",
    "    lambda x: sum(eval(x)) / len(eval(x)) if isinstance(x, str) and len(eval(x)) > 0 else 0\n",
    ")\n",
    "node_df['rating_count'] = node_df['ratings'].apply(\n",
    "    lambda x: len(eval(x)) if isinstance(x, str) else 0\n",
    ")\n",
    "\n",
    "print(\"Average rating and rating count calculated\")\n",
    "display(node_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Vo4CGwTF1uY"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Remove duplicate node IDs from edge columns\n",
    "# ============================================================================\n",
    "\n",
    "edge_columns = ['also_buy', 'also_view', 'has_brand', 'has_category', 'has_color']\n",
    "\n",
    "for col in edge_columns:\n",
    "    if col in node_df.columns:\n",
    "        node_df[col] = node_df[col].apply(\n",
    "            lambda x: list(set(eval(x))) if isinstance(x, str) else []\n",
    "        )\n",
    "\n",
    "print(\"Duplicates removed from edge data\")\n",
    "display(node_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-LdYwzDU2fF"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Create combined_text column\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Creating combined_text column for {len(node_df)} rows...\")\n",
    "\n",
    "def create_combined_text(row):\n",
    "    \"\"\"\n",
    "    Combine all relevant text fields into a single text column.\n",
    "    Format matches existing file: \"Field: value. Field: value. ...\"\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "\n",
    "    # Title\n",
    "    if pd.notna(row.get('title')) and str(row['title']).strip():\n",
    "        parts.append(f\"Title: {row['title']}\")\n",
    "\n",
    "    # Description\n",
    "    if pd.notna(row.get('description')) and str(row['description']).strip():\n",
    "        parts.append(f\"Description: {row['description']}\")\n",
    "\n",
    "    # Feature\n",
    "    if pd.notna(row.get('feature')) and str(row['feature']).strip():\n",
    "        parts.append(f\"Features: {row['feature']}\")\n",
    "\n",
    "    # Brand\n",
    "    if pd.notna(row.get('brand')) and str(row['brand']).strip():\n",
    "        parts.append(f\"Brand: {row['brand']}\")\n",
    "\n",
    "    # Reviews\n",
    "    if pd.notna(row.get('reviews')) and str(row['reviews']).strip():\n",
    "        parts.append(f\"Reviews: {row['reviews']}\")\n",
    "\n",
    "    # Price\n",
    "    if pd.notna(row.get('price')):\n",
    "        parts.append(f\"Price: {row['price']}\")\n",
    "\n",
    "    # Global Category\n",
    "    if pd.notna(row.get('global_category')) and str(row['global_category']).strip():\n",
    "        parts.append(f\"Global Category: {row['global_category']}\")\n",
    "\n",
    "    # Categories\n",
    "    if pd.notna(row.get('categories')) and str(row['categories']).strip():\n",
    "        parts.append(f\"Categories: {row['categories']}\")\n",
    "\n",
    "    # Rank\n",
    "    if pd.notna(row.get('rank')) and str(row['rank']).strip() and str(row['rank']) != 'Unknown':\n",
    "        parts.append(f\"Rank: {row['rank']}\")\n",
    "\n",
    "    # Average Rating\n",
    "    if pd.notna(row.get('avg_rating')) and row['avg_rating'] > 0:\n",
    "        parts.append(f\"Rating: {row['avg_rating']}\")\n",
    "\n",
    "    # Join all parts with \". \"\n",
    "    return \". \".join(parts)\n",
    "\n",
    "# Apply the function to create combined_text\n",
    "node_df['combined_text'] = node_df.apply(create_combined_text, axis=1)\n",
    "\n",
    "print(f\"âœ… Created combined_text column\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\nSample combined_text from new file:\")\n",
    "print(node_df['combined_text'].iloc[0])\n",
    "\n",
    "print(f\"\\nLength statistics:\")\n",
    "print(f\"Avg length: {node_df['combined_text'].str.len().mean():.0f} chars\")\n",
    "print(f\"Max length: {node_df['combined_text'].str.len().max():.0f} chars\")\n",
    "print(f\"Min length: {node_df['combined_text'].str.len().min():.0f} chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ajah2VmOvUx"
   },
   "outputs": [],
   "source": [
    "# Save the cleaned dataframe\n",
    "output_path = '/content/drive/MyDrive/clean_stark_amazon_skb.csv'\n",
    "node_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_path}\")\n",
    "print(f\"\\nFinal shape: {node_df.shape}\")\n",
    "print(f\"Columns: {list(node_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2eaL3ikQ_I-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
